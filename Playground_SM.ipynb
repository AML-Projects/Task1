{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT THE AGE OF A BRAIN FROM MRI FEATURES\n",
    "\n",
    "This task is primarily concerned with regression. However, we have perturbed the original MRI features in several ways. You will need to perform outlier detection, feature selection, and other preprocessing to achieve the best result.\n",
    "\n",
    "The evaluation metric for this task is the Coefficient of Determination (R^2 ) Score which ranges from minus infinity to 1. \n",
    "\n",
    "`\n",
    "from sklearn.metrics import r2_score\n",
    "score = r2_score(y, y_pred)\n",
    "`\n",
    "\n",
    "Note: Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776, 832)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv(\"./data/X_test.csv\", index_col=0)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1212, 832), (1212, 1))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"./data/X_train.csv\", index_col=0)\n",
    "y_train = pd.read_csv(\"./data/y_train.csv\", index_col=0)\n",
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x822</th>\n",
       "      <th>x823</th>\n",
       "      <th>x824</th>\n",
       "      <th>x825</th>\n",
       "      <th>x826</th>\n",
       "      <th>x827</th>\n",
       "      <th>x828</th>\n",
       "      <th>x829</th>\n",
       "      <th>x830</th>\n",
       "      <th>x831</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>10.891876</td>\n",
       "      <td>832442.812375</td>\n",
       "      <td>20585.544083</td>\n",
       "      <td>1028.369495</td>\n",
       "      <td>1.163780e+06</td>\n",
       "      <td>9.199135</td>\n",
       "      <td>597900.477629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.144294e+06</td>\n",
       "      <td>785176.201298</td>\n",
       "      <td>...</td>\n",
       "      <td>1.024198e+06</td>\n",
       "      <td>-855.549602</td>\n",
       "      <td>12176.073427</td>\n",
       "      <td>10.647729</td>\n",
       "      <td>10.916371</td>\n",
       "      <td>1220.065443</td>\n",
       "      <td>8.566724</td>\n",
       "      <td>1.036263e+06</td>\n",
       "      <td>85338.558539</td>\n",
       "      <td>103088.664210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>11.512994</td>\n",
       "      <td>832442.898114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1012.624877</td>\n",
       "      <td>1.028911e+06</td>\n",
       "      <td>10.906408</td>\n",
       "      <td>597900.458612</td>\n",
       "      <td>8127.016078</td>\n",
       "      <td>1.099166e+06</td>\n",
       "      <td>785176.258299</td>\n",
       "      <td>...</td>\n",
       "      <td>1.086806e+06</td>\n",
       "      <td>-787.397942</td>\n",
       "      <td>10493.095660</td>\n",
       "      <td>10.586492</td>\n",
       "      <td>9.463962</td>\n",
       "      <td>917.094909</td>\n",
       "      <td>10.231822</td>\n",
       "      <td>1.007163e+06</td>\n",
       "      <td>95695.020645</td>\n",
       "      <td>105161.109422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>11.052185</td>\n",
       "      <td>832442.896307</td>\n",
       "      <td>20585.512844</td>\n",
       "      <td>1003.953827</td>\n",
       "      <td>9.231756e+05</td>\n",
       "      <td>9.212979</td>\n",
       "      <td>597900.426764</td>\n",
       "      <td>10738.092422</td>\n",
       "      <td>1.027863e+06</td>\n",
       "      <td>785176.223468</td>\n",
       "      <td>...</td>\n",
       "      <td>1.018533e+06</td>\n",
       "      <td>-906.997242</td>\n",
       "      <td>10959.516944</td>\n",
       "      <td>10.769287</td>\n",
       "      <td>10.342160</td>\n",
       "      <td>637.027802</td>\n",
       "      <td>10.705461</td>\n",
       "      <td>1.019955e+06</td>\n",
       "      <td>80253.299882</td>\n",
       "      <td>104177.051666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>11.642076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1004.672084</td>\n",
       "      <td>9.459461e+05</td>\n",
       "      <td>9.553420</td>\n",
       "      <td>597900.450367</td>\n",
       "      <td>13524.096973</td>\n",
       "      <td>1.168144e+06</td>\n",
       "      <td>785176.254867</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047017e+06</td>\n",
       "      <td>-1011.742516</td>\n",
       "      <td>16845.309819</td>\n",
       "      <td>10.483830</td>\n",
       "      <td>10.594941</td>\n",
       "      <td>1114.069590</td>\n",
       "      <td>10.321063</td>\n",
       "      <td>1.085442e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102746.516920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>10.407121</td>\n",
       "      <td>832442.831424</td>\n",
       "      <td>20585.557007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.957182e+05</td>\n",
       "      <td>8.419164</td>\n",
       "      <td>597900.423639</td>\n",
       "      <td>12894.065081</td>\n",
       "      <td>1.063199e+06</td>\n",
       "      <td>785176.190880</td>\n",
       "      <td>...</td>\n",
       "      <td>1.031009e+06</td>\n",
       "      <td>-1025.223865</td>\n",
       "      <td>18348.460040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1230.088215</td>\n",
       "      <td>10.250096</td>\n",
       "      <td>1.024812e+06</td>\n",
       "      <td>101815.745499</td>\n",
       "      <td>105163.749149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x0             x1            x2           x3            x4  \\\n",
       "id                                                                       \n",
       "0.0  10.891876  832442.812375  20585.544083  1028.369495  1.163780e+06   \n",
       "1.0  11.512994  832442.898114           NaN  1012.624877  1.028911e+06   \n",
       "2.0  11.052185  832442.896307  20585.512844  1003.953827  9.231756e+05   \n",
       "3.0  11.642076            NaN           NaN  1004.672084  9.459461e+05   \n",
       "4.0  10.407121  832442.831424  20585.557007          NaN  9.957182e+05   \n",
       "\n",
       "            x5             x6            x7            x8             x9  ...  \\\n",
       "id                                                                        ...   \n",
       "0.0   9.199135  597900.477629           NaN  1.144294e+06  785176.201298  ...   \n",
       "1.0  10.906408  597900.458612   8127.016078  1.099166e+06  785176.258299  ...   \n",
       "2.0   9.212979  597900.426764  10738.092422  1.027863e+06  785176.223468  ...   \n",
       "3.0   9.553420  597900.450367  13524.096973  1.168144e+06  785176.254867  ...   \n",
       "4.0   8.419164  597900.423639  12894.065081  1.063199e+06  785176.190880  ...   \n",
       "\n",
       "             x822         x823          x824       x825       x826  \\\n",
       "id                                                                   \n",
       "0.0  1.024198e+06  -855.549602  12176.073427  10.647729  10.916371   \n",
       "1.0  1.086806e+06  -787.397942  10493.095660  10.586492   9.463962   \n",
       "2.0  1.018533e+06  -906.997242  10959.516944  10.769287  10.342160   \n",
       "3.0  1.047017e+06 -1011.742516  16845.309819  10.483830  10.594941   \n",
       "4.0  1.031009e+06 -1025.223865  18348.460040        NaN        NaN   \n",
       "\n",
       "            x827       x828          x829           x830           x831  \n",
       "id                                                                       \n",
       "0.0  1220.065443   8.566724  1.036263e+06   85338.558539  103088.664210  \n",
       "1.0   917.094909  10.231822  1.007163e+06   95695.020645  105161.109422  \n",
       "2.0   637.027802  10.705461  1.019955e+06   80253.299882  104177.051666  \n",
       "3.0  1114.069590  10.321063  1.085442e+06            NaN  102746.516920  \n",
       "4.0  1230.088215  10.250096  1.024812e+06  101815.745499  105163.749149  \n",
       "\n",
       "[5 rows x 832 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        y\n",
       "id       \n",
       "0.0  71.0\n",
       "1.0  73.0\n",
       "2.0  66.0\n",
       "3.0  55.0\n",
       "4.0  67.0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x822</th>\n",
       "      <th>x823</th>\n",
       "      <th>x824</th>\n",
       "      <th>x825</th>\n",
       "      <th>x826</th>\n",
       "      <th>x827</th>\n",
       "      <th>x828</th>\n",
       "      <th>x829</th>\n",
       "      <th>x830</th>\n",
       "      <th>x831</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>9.101943</td>\n",
       "      <td>832442.839400</td>\n",
       "      <td>20585.538590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.005104e+06</td>\n",
       "      <td>10.417525</td>\n",
       "      <td>597900.393691</td>\n",
       "      <td>10791.058148</td>\n",
       "      <td>9.829612e+05</td>\n",
       "      <td>785176.180458</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049199e+06</td>\n",
       "      <td>-1002.286934</td>\n",
       "      <td>11388.219873</td>\n",
       "      <td>10.538011</td>\n",
       "      <td>9.070287</td>\n",
       "      <td>1122.095390</td>\n",
       "      <td>9.590537</td>\n",
       "      <td>1.006411e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106668.615874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>11.722077</td>\n",
       "      <td>832442.826314</td>\n",
       "      <td>20585.509289</td>\n",
       "      <td>1039.097880</td>\n",
       "      <td>9.836399e+05</td>\n",
       "      <td>11.185000</td>\n",
       "      <td>597900.474419</td>\n",
       "      <td>10321.030961</td>\n",
       "      <td>9.998527e+05</td>\n",
       "      <td>785176.200371</td>\n",
       "      <td>...</td>\n",
       "      <td>1.063850e+06</td>\n",
       "      <td>-777.155703</td>\n",
       "      <td>15454.964631</td>\n",
       "      <td>10.438084</td>\n",
       "      <td>10.573977</td>\n",
       "      <td>830.040750</td>\n",
       "      <td>11.283446</td>\n",
       "      <td>1.064455e+06</td>\n",
       "      <td>100761.264268</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>11.200277</td>\n",
       "      <td>832442.820359</td>\n",
       "      <td>20585.511136</td>\n",
       "      <td>1081.926822</td>\n",
       "      <td>1.059913e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>597900.432849</td>\n",
       "      <td>9394.048739</td>\n",
       "      <td>8.439144e+05</td>\n",
       "      <td>785176.185410</td>\n",
       "      <td>...</td>\n",
       "      <td>1.033983e+06</td>\n",
       "      <td>-866.209778</td>\n",
       "      <td>13616.308487</td>\n",
       "      <td>10.123607</td>\n",
       "      <td>8.273391</td>\n",
       "      <td>926.072523</td>\n",
       "      <td>9.277080</td>\n",
       "      <td>1.007427e+06</td>\n",
       "      <td>106440.456728</td>\n",
       "      <td>103405.273232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>9.668873</td>\n",
       "      <td>832442.820901</td>\n",
       "      <td>20585.528528</td>\n",
       "      <td>1063.771791</td>\n",
       "      <td>1.023284e+06</td>\n",
       "      <td>9.635705</td>\n",
       "      <td>597900.426803</td>\n",
       "      <td>10640.030819</td>\n",
       "      <td>1.018818e+06</td>\n",
       "      <td>785176.263405</td>\n",
       "      <td>...</td>\n",
       "      <td>1.064161e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14596.359851</td>\n",
       "      <td>10.756986</td>\n",
       "      <td>8.540542</td>\n",
       "      <td>1067.003746</td>\n",
       "      <td>9.803903</td>\n",
       "      <td>1.076560e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100531.960204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>10.329962</td>\n",
       "      <td>832442.871842</td>\n",
       "      <td>20585.483009</td>\n",
       "      <td>1013.321073</td>\n",
       "      <td>9.396156e+05</td>\n",
       "      <td>10.664417</td>\n",
       "      <td>597900.399834</td>\n",
       "      <td>10464.062039</td>\n",
       "      <td>1.024615e+06</td>\n",
       "      <td>785176.251995</td>\n",
       "      <td>...</td>\n",
       "      <td>1.070782e+06</td>\n",
       "      <td>-1033.312644</td>\n",
       "      <td>14321.185784</td>\n",
       "      <td>10.708867</td>\n",
       "      <td>10.285677</td>\n",
       "      <td>985.074197</td>\n",
       "      <td>9.284969</td>\n",
       "      <td>1.097973e+06</td>\n",
       "      <td>109797.625066</td>\n",
       "      <td>104849.648797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x0             x1            x2           x3            x4  \\\n",
       "id                                                                       \n",
       "0.0   9.101943  832442.839400  20585.538590          NaN  1.005104e+06   \n",
       "1.0  11.722077  832442.826314  20585.509289  1039.097880  9.836399e+05   \n",
       "2.0  11.200277  832442.820359  20585.511136  1081.926822  1.059913e+06   \n",
       "3.0   9.668873  832442.820901  20585.528528  1063.771791  1.023284e+06   \n",
       "4.0  10.329962  832442.871842  20585.483009  1013.321073  9.396156e+05   \n",
       "\n",
       "            x5             x6            x7            x8             x9  ...  \\\n",
       "id                                                                        ...   \n",
       "0.0  10.417525  597900.393691  10791.058148  9.829612e+05  785176.180458  ...   \n",
       "1.0  11.185000  597900.474419  10321.030961  9.998527e+05  785176.200371  ...   \n",
       "2.0        NaN  597900.432849   9394.048739  8.439144e+05  785176.185410  ...   \n",
       "3.0   9.635705  597900.426803  10640.030819  1.018818e+06  785176.263405  ...   \n",
       "4.0  10.664417  597900.399834  10464.062039  1.024615e+06  785176.251995  ...   \n",
       "\n",
       "             x822         x823          x824       x825       x826  \\\n",
       "id                                                                   \n",
       "0.0  1.049199e+06 -1002.286934  11388.219873  10.538011   9.070287   \n",
       "1.0  1.063850e+06  -777.155703  15454.964631  10.438084  10.573977   \n",
       "2.0  1.033983e+06  -866.209778  13616.308487  10.123607   8.273391   \n",
       "3.0  1.064161e+06          NaN  14596.359851  10.756986   8.540542   \n",
       "4.0  1.070782e+06 -1033.312644  14321.185784  10.708867  10.285677   \n",
       "\n",
       "            x827       x828          x829           x830           x831  \n",
       "id                                                                       \n",
       "0.0  1122.095390   9.590537  1.006411e+06            NaN  106668.615874  \n",
       "1.0   830.040750  11.283446  1.064455e+06  100761.264268            NaN  \n",
       "2.0   926.072523   9.277080  1.007427e+06  106440.456728  103405.273232  \n",
       "3.0  1067.003746   9.803903  1.076560e+06            NaN  100531.960204  \n",
       "4.0   985.074197   9.284969  1.097973e+06  109797.625066  104849.648797  \n",
       "\n",
       "[5 rows x 832 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the test set there are missing: 40610\n"
     ]
    }
   ],
   "source": [
    "print('In the test set there are missing: %d' % sum(np.isnan(X_test.values).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x822</th>\n",
       "      <th>x823</th>\n",
       "      <th>x824</th>\n",
       "      <th>x825</th>\n",
       "      <th>x826</th>\n",
       "      <th>x827</th>\n",
       "      <th>x828</th>\n",
       "      <th>x829</th>\n",
       "      <th>x830</th>\n",
       "      <th>x831</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1118.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1117.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>1.117000e+03</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1105.000000</td>\n",
       "      <td>1127.000000</td>\n",
       "      <td>1.116000e+03</td>\n",
       "      <td>1124.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.134000e+03</td>\n",
       "      <td>1125.000000</td>\n",
       "      <td>1098.000000</td>\n",
       "      <td>1121.000000</td>\n",
       "      <td>1120.000000</td>\n",
       "      <td>1109.000000</td>\n",
       "      <td>1115.000000</td>\n",
       "      <td>1.112000e+03</td>\n",
       "      <td>1124.000000</td>\n",
       "      <td>1091.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.026057</td>\n",
       "      <td>832442.859290</td>\n",
       "      <td>20585.524887</td>\n",
       "      <td>1048.958235</td>\n",
       "      <td>1.000291e+06</td>\n",
       "      <td>10.085010</td>\n",
       "      <td>597900.429955</td>\n",
       "      <td>10389.657239</td>\n",
       "      <td>9.998422e+05</td>\n",
       "      <td>785176.225858</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049674e+06</td>\n",
       "      <td>-876.044006</td>\n",
       "      <td>13492.600186</td>\n",
       "      <td>10.554762</td>\n",
       "      <td>10.057767</td>\n",
       "      <td>1066.141107</td>\n",
       "      <td>10.008269</td>\n",
       "      <td>1.050199e+06</td>\n",
       "      <td>99798.480171</td>\n",
       "      <td>104903.905758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.968347</td>\n",
       "      <td>0.028258</td>\n",
       "      <td>0.029051</td>\n",
       "      <td>28.430733</td>\n",
       "      <td>9.740891e+04</td>\n",
       "      <td>0.968026</td>\n",
       "      <td>0.028128</td>\n",
       "      <td>1655.843472</td>\n",
       "      <td>1.022441e+05</td>\n",
       "      <td>0.028799</td>\n",
       "      <td>...</td>\n",
       "      <td>2.839579e+04</td>\n",
       "      <td>164.585576</td>\n",
       "      <td>2519.835006</td>\n",
       "      <td>0.283844</td>\n",
       "      <td>0.982656</td>\n",
       "      <td>226.606986</td>\n",
       "      <td>1.018930</td>\n",
       "      <td>2.814210e+04</td>\n",
       "      <td>9576.128720</td>\n",
       "      <td>2768.405350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.672068</td>\n",
       "      <td>832442.808579</td>\n",
       "      <td>20585.473809</td>\n",
       "      <td>1000.063783</td>\n",
       "      <td>6.800215e+05</td>\n",
       "      <td>6.984052</td>\n",
       "      <td>597900.381003</td>\n",
       "      <td>3644.074892</td>\n",
       "      <td>6.095730e+05</td>\n",
       "      <td>785176.176297</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000105e+06</td>\n",
       "      <td>-1597.766964</td>\n",
       "      <td>2536.030655</td>\n",
       "      <td>10.010366</td>\n",
       "      <td>6.841039</td>\n",
       "      <td>496.007706</td>\n",
       "      <td>6.466963</td>\n",
       "      <td>1.000002e+06</td>\n",
       "      <td>73207.994891</td>\n",
       "      <td>100012.896777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.381273</td>\n",
       "      <td>832442.835941</td>\n",
       "      <td>20585.501013</td>\n",
       "      <td>1024.969967</td>\n",
       "      <td>9.360882e+05</td>\n",
       "      <td>9.470582</td>\n",
       "      <td>597900.406110</td>\n",
       "      <td>9339.537887</td>\n",
       "      <td>9.322937e+05</td>\n",
       "      <td>785176.201279</td>\n",
       "      <td>...</td>\n",
       "      <td>1.025054e+06</td>\n",
       "      <td>-975.398714</td>\n",
       "      <td>11947.954006</td>\n",
       "      <td>10.321039</td>\n",
       "      <td>9.379001</td>\n",
       "      <td>899.067501</td>\n",
       "      <td>9.325229</td>\n",
       "      <td>1.027575e+06</td>\n",
       "      <td>93416.252400</td>\n",
       "      <td>102596.190683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.000079</td>\n",
       "      <td>832442.860041</td>\n",
       "      <td>20585.524817</td>\n",
       "      <td>1047.985497</td>\n",
       "      <td>1.000557e+06</td>\n",
       "      <td>10.089601</td>\n",
       "      <td>597900.429787</td>\n",
       "      <td>10295.013382</td>\n",
       "      <td>1.001261e+06</td>\n",
       "      <td>785176.225608</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049296e+06</td>\n",
       "      <td>-875.508235</td>\n",
       "      <td>13352.186179</td>\n",
       "      <td>10.554260</td>\n",
       "      <td>10.114370</td>\n",
       "      <td>1049.027077</td>\n",
       "      <td>10.005684</td>\n",
       "      <td>1.050262e+06</td>\n",
       "      <td>99802.127899</td>\n",
       "      <td>104846.235709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.664998</td>\n",
       "      <td>832442.882951</td>\n",
       "      <td>20585.550525</td>\n",
       "      <td>1073.180317</td>\n",
       "      <td>1.064617e+06</td>\n",
       "      <td>10.752707</td>\n",
       "      <td>597900.452983</td>\n",
       "      <td>11304.073469</td>\n",
       "      <td>1.068359e+06</td>\n",
       "      <td>785176.250421</td>\n",
       "      <td>...</td>\n",
       "      <td>1.074354e+06</td>\n",
       "      <td>-773.174562</td>\n",
       "      <td>14893.726023</td>\n",
       "      <td>10.792195</td>\n",
       "      <td>10.745370</td>\n",
       "      <td>1215.057985</td>\n",
       "      <td>10.658120</td>\n",
       "      <td>1.073831e+06</td>\n",
       "      <td>106400.748441</td>\n",
       "      <td>107098.669350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.956099</td>\n",
       "      <td>832442.908334</td>\n",
       "      <td>20585.573514</td>\n",
       "      <td>1099.977638</td>\n",
       "      <td>1.331630e+06</td>\n",
       "      <td>12.747734</td>\n",
       "      <td>597900.480810</td>\n",
       "      <td>17347.531573</td>\n",
       "      <td>1.284804e+06</td>\n",
       "      <td>785176.276168</td>\n",
       "      <td>...</td>\n",
       "      <td>1.099771e+06</td>\n",
       "      <td>-281.030205</td>\n",
       "      <td>24815.260375</td>\n",
       "      <td>11.091050</td>\n",
       "      <td>13.530204</td>\n",
       "      <td>2122.032859</td>\n",
       "      <td>13.163113</td>\n",
       "      <td>1.099918e+06</td>\n",
       "      <td>130694.436443</td>\n",
       "      <td>109984.169649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                x0             x1            x2           x3            x4  \\\n",
       "count  1118.000000    1114.000000   1117.000000  1106.000000  1.117000e+03   \n",
       "mean     10.026057  832442.859290  20585.524887  1048.958235  1.000291e+06   \n",
       "std       0.968347       0.028258      0.029051    28.430733  9.740891e+04   \n",
       "min       6.672068  832442.808579  20585.473809  1000.063783  6.800215e+05   \n",
       "25%       9.381273  832442.835941  20585.501013  1024.969967  9.360882e+05   \n",
       "50%      10.000079  832442.860041  20585.524817  1047.985497  1.000557e+06   \n",
       "75%      10.664998  832442.882951  20585.550525  1073.180317  1.064617e+06   \n",
       "max      12.956099  832442.908334  20585.573514  1099.977638  1.331630e+06   \n",
       "\n",
       "                x5             x6            x7            x8             x9  \\\n",
       "count  1128.000000    1105.000000   1127.000000  1.116000e+03    1124.000000   \n",
       "mean     10.085010  597900.429955  10389.657239  9.998422e+05  785176.225858   \n",
       "std       0.968026       0.028128   1655.843472  1.022441e+05       0.028799   \n",
       "min       6.984052  597900.381003   3644.074892  6.095730e+05  785176.176297   \n",
       "25%       9.470582  597900.406110   9339.537887  9.322937e+05  785176.201279   \n",
       "50%      10.089601  597900.429787  10295.013382  1.001261e+06  785176.225608   \n",
       "75%      10.752707  597900.452983  11304.073469  1.068359e+06  785176.250421   \n",
       "max      12.747734  597900.480810  17347.531573  1.284804e+06  785176.276168   \n",
       "\n",
       "       ...          x822         x823          x824         x825         x826  \\\n",
       "count  ...  1.134000e+03  1125.000000   1098.000000  1121.000000  1120.000000   \n",
       "mean   ...  1.049674e+06  -876.044006  13492.600186    10.554762    10.057767   \n",
       "std    ...  2.839579e+04   164.585576   2519.835006     0.283844     0.982656   \n",
       "min    ...  1.000105e+06 -1597.766964   2536.030655    10.010366     6.841039   \n",
       "25%    ...  1.025054e+06  -975.398714  11947.954006    10.321039     9.379001   \n",
       "50%    ...  1.049296e+06  -875.508235  13352.186179    10.554260    10.114370   \n",
       "75%    ...  1.074354e+06  -773.174562  14893.726023    10.792195    10.745370   \n",
       "max    ...  1.099771e+06  -281.030205  24815.260375    11.091050    13.530204   \n",
       "\n",
       "              x827         x828          x829           x830           x831  \n",
       "count  1109.000000  1115.000000  1.112000e+03    1124.000000    1091.000000  \n",
       "mean   1066.141107    10.008269  1.050199e+06   99798.480171  104903.905758  \n",
       "std     226.606986     1.018930  2.814210e+04    9576.128720    2768.405350  \n",
       "min     496.007706     6.466963  1.000002e+06   73207.994891  100012.896777  \n",
       "25%     899.067501     9.325229  1.027575e+06   93416.252400  102596.190683  \n",
       "50%    1049.027077    10.005684  1.050262e+06   99802.127899  104846.235709  \n",
       "75%    1215.057985    10.658120  1.073831e+06  106400.748441  107098.669350  \n",
       "max    2122.032859    13.163113  1.099918e+06  130694.436443  109984.169649  \n",
       "\n",
       "[8 rows x 832 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 76910\n"
     ]
    }
   ],
   "source": [
    "print('Missing: %d' % sum(np.isnan(X_train.values).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is missing values in the training set. Question: are there missing values in the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "print('Missing: %d' % sum(np.isnan(y_train.values).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values in the training labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "Todo:\n",
    "- deal with missing data (NAN) (https://machinelearningmastery.com/handle-missing-data-python/ )  \n",
    "- outlier detection, remove outliers (https://machinelearningmastery.com/model-based-outlier-detection-and-removal-in-python/)\n",
    "- normalize (standardnormalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) Deal with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.) Remove Rows With Missing Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 832)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_subset = X_train.dropna(inplace=False)\n",
    "X_train_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we cannot use this strategy, because all rows are removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.) Impute Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b.1) replace missing values with the mean value for each column\n",
    "Could also try median or mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1212, 832)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mean_imputed = X_train.fillna(X_train.mean(), inplace=False)\n",
    "X_train_mean_imputed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x822</th>\n",
       "      <th>x823</th>\n",
       "      <th>x824</th>\n",
       "      <th>x825</th>\n",
       "      <th>x826</th>\n",
       "      <th>x827</th>\n",
       "      <th>x828</th>\n",
       "      <th>x829</th>\n",
       "      <th>x830</th>\n",
       "      <th>x831</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>10.891876</td>\n",
       "      <td>832442.812375</td>\n",
       "      <td>20585.544083</td>\n",
       "      <td>1028.369495</td>\n",
       "      <td>1.163780e+06</td>\n",
       "      <td>9.199135</td>\n",
       "      <td>597900.477629</td>\n",
       "      <td>10389.657239</td>\n",
       "      <td>1.144294e+06</td>\n",
       "      <td>785176.201298</td>\n",
       "      <td>...</td>\n",
       "      <td>1.024198e+06</td>\n",
       "      <td>-855.549602</td>\n",
       "      <td>12176.073427</td>\n",
       "      <td>10.647729</td>\n",
       "      <td>10.916371</td>\n",
       "      <td>1220.065443</td>\n",
       "      <td>8.566724</td>\n",
       "      <td>1.036263e+06</td>\n",
       "      <td>85338.558539</td>\n",
       "      <td>103088.664210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>11.512994</td>\n",
       "      <td>832442.898114</td>\n",
       "      <td>20585.524887</td>\n",
       "      <td>1012.624877</td>\n",
       "      <td>1.028911e+06</td>\n",
       "      <td>10.906408</td>\n",
       "      <td>597900.458612</td>\n",
       "      <td>8127.016078</td>\n",
       "      <td>1.099166e+06</td>\n",
       "      <td>785176.258299</td>\n",
       "      <td>...</td>\n",
       "      <td>1.086806e+06</td>\n",
       "      <td>-787.397942</td>\n",
       "      <td>10493.095660</td>\n",
       "      <td>10.586492</td>\n",
       "      <td>9.463962</td>\n",
       "      <td>917.094909</td>\n",
       "      <td>10.231822</td>\n",
       "      <td>1.007163e+06</td>\n",
       "      <td>95695.020645</td>\n",
       "      <td>105161.109422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>11.052185</td>\n",
       "      <td>832442.896307</td>\n",
       "      <td>20585.512844</td>\n",
       "      <td>1003.953827</td>\n",
       "      <td>9.231756e+05</td>\n",
       "      <td>9.212979</td>\n",
       "      <td>597900.426764</td>\n",
       "      <td>10738.092422</td>\n",
       "      <td>1.027863e+06</td>\n",
       "      <td>785176.223468</td>\n",
       "      <td>...</td>\n",
       "      <td>1.018533e+06</td>\n",
       "      <td>-906.997242</td>\n",
       "      <td>10959.516944</td>\n",
       "      <td>10.769287</td>\n",
       "      <td>10.342160</td>\n",
       "      <td>637.027802</td>\n",
       "      <td>10.705461</td>\n",
       "      <td>1.019955e+06</td>\n",
       "      <td>80253.299882</td>\n",
       "      <td>104177.051666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>11.642076</td>\n",
       "      <td>832442.859290</td>\n",
       "      <td>20585.524887</td>\n",
       "      <td>1004.672084</td>\n",
       "      <td>9.459461e+05</td>\n",
       "      <td>9.553420</td>\n",
       "      <td>597900.450367</td>\n",
       "      <td>13524.096973</td>\n",
       "      <td>1.168144e+06</td>\n",
       "      <td>785176.254867</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047017e+06</td>\n",
       "      <td>-1011.742516</td>\n",
       "      <td>16845.309819</td>\n",
       "      <td>10.483830</td>\n",
       "      <td>10.594941</td>\n",
       "      <td>1114.069590</td>\n",
       "      <td>10.321063</td>\n",
       "      <td>1.085442e+06</td>\n",
       "      <td>99798.480171</td>\n",
       "      <td>102746.516920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>10.407121</td>\n",
       "      <td>832442.831424</td>\n",
       "      <td>20585.557007</td>\n",
       "      <td>1048.958235</td>\n",
       "      <td>9.957182e+05</td>\n",
       "      <td>8.419164</td>\n",
       "      <td>597900.423639</td>\n",
       "      <td>12894.065081</td>\n",
       "      <td>1.063199e+06</td>\n",
       "      <td>785176.190880</td>\n",
       "      <td>...</td>\n",
       "      <td>1.031009e+06</td>\n",
       "      <td>-1025.223865</td>\n",
       "      <td>18348.460040</td>\n",
       "      <td>10.554762</td>\n",
       "      <td>10.057767</td>\n",
       "      <td>1230.088215</td>\n",
       "      <td>10.250096</td>\n",
       "      <td>1.024812e+06</td>\n",
       "      <td>101815.745499</td>\n",
       "      <td>105163.749149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x0             x1            x2           x3            x4  \\\n",
       "id                                                                       \n",
       "0.0  10.891876  832442.812375  20585.544083  1028.369495  1.163780e+06   \n",
       "1.0  11.512994  832442.898114  20585.524887  1012.624877  1.028911e+06   \n",
       "2.0  11.052185  832442.896307  20585.512844  1003.953827  9.231756e+05   \n",
       "3.0  11.642076  832442.859290  20585.524887  1004.672084  9.459461e+05   \n",
       "4.0  10.407121  832442.831424  20585.557007  1048.958235  9.957182e+05   \n",
       "\n",
       "            x5             x6            x7            x8             x9  ...  \\\n",
       "id                                                                        ...   \n",
       "0.0   9.199135  597900.477629  10389.657239  1.144294e+06  785176.201298  ...   \n",
       "1.0  10.906408  597900.458612   8127.016078  1.099166e+06  785176.258299  ...   \n",
       "2.0   9.212979  597900.426764  10738.092422  1.027863e+06  785176.223468  ...   \n",
       "3.0   9.553420  597900.450367  13524.096973  1.168144e+06  785176.254867  ...   \n",
       "4.0   8.419164  597900.423639  12894.065081  1.063199e+06  785176.190880  ...   \n",
       "\n",
       "             x822         x823          x824       x825       x826  \\\n",
       "id                                                                   \n",
       "0.0  1.024198e+06  -855.549602  12176.073427  10.647729  10.916371   \n",
       "1.0  1.086806e+06  -787.397942  10493.095660  10.586492   9.463962   \n",
       "2.0  1.018533e+06  -906.997242  10959.516944  10.769287  10.342160   \n",
       "3.0  1.047017e+06 -1011.742516  16845.309819  10.483830  10.594941   \n",
       "4.0  1.031009e+06 -1025.223865  18348.460040  10.554762  10.057767   \n",
       "\n",
       "            x827       x828          x829           x830           x831  \n",
       "id                                                                       \n",
       "0.0  1220.065443   8.566724  1.036263e+06   85338.558539  103088.664210  \n",
       "1.0   917.094909  10.231822  1.007163e+06   95695.020645  105161.109422  \n",
       "2.0   637.027802  10.705461  1.019955e+06   80253.299882  104177.051666  \n",
       "3.0  1114.069590  10.321063  1.085442e+06   99798.480171  102746.516920  \n",
       "4.0  1230.088215  10.250096  1.024812e+06  101815.745499  105163.749149  \n",
       "\n",
       "[5 rows x 832 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mean_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x822</th>\n",
       "      <th>x823</th>\n",
       "      <th>x824</th>\n",
       "      <th>x825</th>\n",
       "      <th>x826</th>\n",
       "      <th>x827</th>\n",
       "      <th>x828</th>\n",
       "      <th>x829</th>\n",
       "      <th>x830</th>\n",
       "      <th>x831</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1.212000e+03</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1.212000e+03</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.212000e+03</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1.212000e+03</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1212.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.026057</td>\n",
       "      <td>832442.859290</td>\n",
       "      <td>20585.524887</td>\n",
       "      <td>1048.958235</td>\n",
       "      <td>1.000291e+06</td>\n",
       "      <td>10.085010</td>\n",
       "      <td>597900.429955</td>\n",
       "      <td>10389.657239</td>\n",
       "      <td>9.998422e+05</td>\n",
       "      <td>785176.225858</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049674e+06</td>\n",
       "      <td>-876.044006</td>\n",
       "      <td>13492.600186</td>\n",
       "      <td>10.554762</td>\n",
       "      <td>10.057767</td>\n",
       "      <td>1066.141107</td>\n",
       "      <td>10.008269</td>\n",
       "      <td>1.050199e+06</td>\n",
       "      <td>99798.480171</td>\n",
       "      <td>104903.905758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.930005</td>\n",
       "      <td>0.027090</td>\n",
       "      <td>0.027888</td>\n",
       "      <td>27.157959</td>\n",
       "      <td>9.351014e+04</td>\n",
       "      <td>0.933849</td>\n",
       "      <td>0.026856</td>\n",
       "      <td>1596.674554</td>\n",
       "      <td>9.810785e+04</td>\n",
       "      <td>0.027733</td>\n",
       "      <td>...</td>\n",
       "      <td>2.746609e+04</td>\n",
       "      <td>158.563365</td>\n",
       "      <td>2398.299115</td>\n",
       "      <td>0.272971</td>\n",
       "      <td>0.944592</td>\n",
       "      <td>216.755988</td>\n",
       "      <td>0.977271</td>\n",
       "      <td>2.695513e+04</td>\n",
       "      <td>9221.631883</td>\n",
       "      <td>2626.460400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.672068</td>\n",
       "      <td>832442.808579</td>\n",
       "      <td>20585.473809</td>\n",
       "      <td>1000.063783</td>\n",
       "      <td>6.800215e+05</td>\n",
       "      <td>6.984052</td>\n",
       "      <td>597900.381003</td>\n",
       "      <td>3644.074892</td>\n",
       "      <td>6.095730e+05</td>\n",
       "      <td>785176.176297</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000105e+06</td>\n",
       "      <td>-1597.766964</td>\n",
       "      <td>2536.030655</td>\n",
       "      <td>10.010366</td>\n",
       "      <td>6.841039</td>\n",
       "      <td>496.007706</td>\n",
       "      <td>6.466963</td>\n",
       "      <td>1.000002e+06</td>\n",
       "      <td>73207.994891</td>\n",
       "      <td>100012.896777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.440852</td>\n",
       "      <td>832442.838507</td>\n",
       "      <td>20585.503515</td>\n",
       "      <td>1027.518864</td>\n",
       "      <td>9.427006e+05</td>\n",
       "      <td>9.518125</td>\n",
       "      <td>597900.408556</td>\n",
       "      <td>9409.300050</td>\n",
       "      <td>9.430492e+05</td>\n",
       "      <td>785176.202866</td>\n",
       "      <td>...</td>\n",
       "      <td>1.027129e+06</td>\n",
       "      <td>-965.633580</td>\n",
       "      <td>12164.233176</td>\n",
       "      <td>10.338268</td>\n",
       "      <td>9.460857</td>\n",
       "      <td>914.763005</td>\n",
       "      <td>9.384658</td>\n",
       "      <td>1.029501e+06</td>\n",
       "      <td>94149.196760</td>\n",
       "      <td>102892.311899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.026057</td>\n",
       "      <td>832442.859290</td>\n",
       "      <td>20585.524887</td>\n",
       "      <td>1048.958235</td>\n",
       "      <td>1.000291e+06</td>\n",
       "      <td>10.085010</td>\n",
       "      <td>597900.429955</td>\n",
       "      <td>10389.657239</td>\n",
       "      <td>9.998422e+05</td>\n",
       "      <td>785176.225858</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049674e+06</td>\n",
       "      <td>-876.044006</td>\n",
       "      <td>13492.600186</td>\n",
       "      <td>10.554762</td>\n",
       "      <td>10.057767</td>\n",
       "      <td>1066.141107</td>\n",
       "      <td>10.008269</td>\n",
       "      <td>1.050199e+06</td>\n",
       "      <td>99798.480171</td>\n",
       "      <td>104903.905758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.624582</td>\n",
       "      <td>832442.880460</td>\n",
       "      <td>20585.547770</td>\n",
       "      <td>1070.599035</td>\n",
       "      <td>1.057470e+06</td>\n",
       "      <td>10.689486</td>\n",
       "      <td>597900.451121</td>\n",
       "      <td>11212.573544</td>\n",
       "      <td>1.060764e+06</td>\n",
       "      <td>785176.249144</td>\n",
       "      <td>...</td>\n",
       "      <td>1.072653e+06</td>\n",
       "      <td>-785.801099</td>\n",
       "      <td>14700.041808</td>\n",
       "      <td>10.768303</td>\n",
       "      <td>10.663939</td>\n",
       "      <td>1194.283142</td>\n",
       "      <td>10.603448</td>\n",
       "      <td>1.071379e+06</td>\n",
       "      <td>105761.210966</td>\n",
       "      <td>106826.950763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.956099</td>\n",
       "      <td>832442.908334</td>\n",
       "      <td>20585.573514</td>\n",
       "      <td>1099.977638</td>\n",
       "      <td>1.331630e+06</td>\n",
       "      <td>12.747734</td>\n",
       "      <td>597900.480810</td>\n",
       "      <td>17347.531573</td>\n",
       "      <td>1.284804e+06</td>\n",
       "      <td>785176.276168</td>\n",
       "      <td>...</td>\n",
       "      <td>1.099771e+06</td>\n",
       "      <td>-281.030205</td>\n",
       "      <td>24815.260375</td>\n",
       "      <td>11.091050</td>\n",
       "      <td>13.530204</td>\n",
       "      <td>2122.032859</td>\n",
       "      <td>13.163113</td>\n",
       "      <td>1.099918e+06</td>\n",
       "      <td>130694.436443</td>\n",
       "      <td>109984.169649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                x0             x1            x2           x3            x4  \\\n",
       "count  1212.000000    1212.000000   1212.000000  1212.000000  1.212000e+03   \n",
       "mean     10.026057  832442.859290  20585.524887  1048.958235  1.000291e+06   \n",
       "std       0.930005       0.027090      0.027888    27.157959  9.351014e+04   \n",
       "min       6.672068  832442.808579  20585.473809  1000.063783  6.800215e+05   \n",
       "25%       9.440852  832442.838507  20585.503515  1027.518864  9.427006e+05   \n",
       "50%      10.026057  832442.859290  20585.524887  1048.958235  1.000291e+06   \n",
       "75%      10.624582  832442.880460  20585.547770  1070.599035  1.057470e+06   \n",
       "max      12.956099  832442.908334  20585.573514  1099.977638  1.331630e+06   \n",
       "\n",
       "                x5             x6            x7            x8             x9  \\\n",
       "count  1212.000000    1212.000000   1212.000000  1.212000e+03    1212.000000   \n",
       "mean     10.085010  597900.429955  10389.657239  9.998422e+05  785176.225858   \n",
       "std       0.933849       0.026856   1596.674554  9.810785e+04       0.027733   \n",
       "min       6.984052  597900.381003   3644.074892  6.095730e+05  785176.176297   \n",
       "25%       9.518125  597900.408556   9409.300050  9.430492e+05  785176.202866   \n",
       "50%      10.085010  597900.429955  10389.657239  9.998422e+05  785176.225858   \n",
       "75%      10.689486  597900.451121  11212.573544  1.060764e+06  785176.249144   \n",
       "max      12.747734  597900.480810  17347.531573  1.284804e+06  785176.276168   \n",
       "\n",
       "       ...          x822         x823          x824         x825         x826  \\\n",
       "count  ...  1.212000e+03  1212.000000   1212.000000  1212.000000  1212.000000   \n",
       "mean   ...  1.049674e+06  -876.044006  13492.600186    10.554762    10.057767   \n",
       "std    ...  2.746609e+04   158.563365   2398.299115     0.272971     0.944592   \n",
       "min    ...  1.000105e+06 -1597.766964   2536.030655    10.010366     6.841039   \n",
       "25%    ...  1.027129e+06  -965.633580  12164.233176    10.338268     9.460857   \n",
       "50%    ...  1.049674e+06  -876.044006  13492.600186    10.554762    10.057767   \n",
       "75%    ...  1.072653e+06  -785.801099  14700.041808    10.768303    10.663939   \n",
       "max    ...  1.099771e+06  -281.030205  24815.260375    11.091050    13.530204   \n",
       "\n",
       "              x827         x828          x829           x830           x831  \n",
       "count  1212.000000  1212.000000  1.212000e+03    1212.000000    1212.000000  \n",
       "mean   1066.141107    10.008269  1.050199e+06   99798.480171  104903.905758  \n",
       "std     216.755988     0.977271  2.695513e+04    9221.631883    2626.460400  \n",
       "min     496.007706     6.466963  1.000002e+06   73207.994891  100012.896777  \n",
       "25%     914.763005     9.384658  1.029501e+06   94149.196760  102892.311899  \n",
       "50%    1066.141107    10.008269  1.050199e+06   99798.480171  104903.905758  \n",
       "75%    1194.283142    10.603448  1.071379e+06  105761.210966  106826.950763  \n",
       "max    2122.032859    13.163113  1.099918e+06  130694.436443  109984.169649  \n",
       "\n",
       "[8 rows x 832 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mean_imputed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(X_train_mean_imputed.isna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are no NAN values anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same but with SimpleImputer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan , strategy='mean')\n",
    "X_train_val = X_train.values\n",
    "mean_imputed_values = imputer.fit_transform(X_train_val)\n",
    "print('Missing: %d' % np.isnan(mean_imputed_values).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b.2) Impute with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan , strategy=\"constant\", fill_value=0)\n",
    "X_train_val = X_train.values\n",
    "zero_imputed_values = imputer.fit_transform(X_train_val)\n",
    "print('Missing: %d' % np.isnan(zero_imputed_values).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) Outlier detection\n",
    "\n",
    "\" Each method will be defined, then fit on the training dataset. The fit model will then predict which examples in the training dataset are outliers and which are not (so-called inliers). The outliers will then be removed from the training dataset, then the model will be fit on the remaining examples and evaluated on the entire test dataset.\n",
    "\n",
    "It would be invalid to fit the outlier detection method on the entire training dataset as this would result in data leakage. That is, the model would have access to data (or information about the data) in the test set not used to train the model. This may result in an optimistic estimate of model performance.\"\n",
    "\n",
    "Therefore this needs to be done in the pipeline, I think before standardscaler or other normalization methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1) Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2) Local Outlier Factor\n",
    "\n",
    "A simple approach to identifying outliers is to locate those examples that are far from the other examples in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "outliers = lof.fit_predict(mean_imputed_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1159, 832) (1159, 1)\n"
     ]
    }
   ],
   "source": [
    " # select all rows that are not outliers\n",
    "mask = outliers != -1\n",
    "X_train_outlierfree, y_train_outlierfree = X_train.values[mask, :], y_train.values[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train_outlierfree.shape, y_train_outlierfree.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 53 outliers \n"
     ]
    }
   ],
   "source": [
    "print(\"there are {} outliers \".format(np.sum(outliers == -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) Normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "\n",
    "#  The pipeline can be used as any other estimator\n",
    "# and avoids leaking the test set into the train set\n",
    "\n",
    "my_score = make_scorer(r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ) Baseline Ridge on mean imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think the problem here is that we impute the whole trainig set first, and then do cross validation \n",
    "# --> leaking the train set into the test set?\n",
    "\n",
    "\n",
    "# ridge = Ridge()\n",
    "# pipeline = Pipeline(steps=[('imputer', imputer),('model', ridge)])\n",
    "# kfold = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# result = cross_val_score(pipeline, X_train, y_train, cv=kfold, scoring=my_score)\n",
    "# print('mean R2 score: %.3f' % result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think the problem here is that we impute the whole trainig set first, and then do cross validation \n",
    "# --> leaking the train set into the test set?\n",
    "\n",
    "\n",
    "# ridge_m1 = Ridge()\n",
    "# param = {\"alpha\" : [1e-15, 1e-4, 1, 20]}\n",
    "# ridge_regression = GridSearchCV(ridge_m1, param, scoring= my_score, cv= 5)\n",
    "# ridge_regression.fit(mean_imputed_values, y_train)\n",
    "\n",
    "# print(ridge_regression.best_params_)\n",
    "# print(ridge_regression.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=-0.497):\n",
      "{'ridge__alpha': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahmorillo/anaconda3/envs/python_hs2020/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=7.57833e-46): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    }
   ],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "ridge = Ridge()\n",
    "pipeline = Pipeline(steps=[('imputer', imputer),('ridge', ridge)])\n",
    "# Parameters of pipelines can be set using â€˜__â€™ separated parameter names:\n",
    "param_grid = {'ridge__alpha': [1e-15, 1e-4, 1, 20, 50, 100]}\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid, n_jobs=-1, cv= 10)\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation : Ill-conditioned matrix. What does this mean? Not full rank? -> feauter selection should be done prior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) Ridge, cross validation, mean imputed data, standard normalization\n",
    "standard normalization : z = (x - u) / s ( u: mean, s:std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.152):\n",
      "{'ridge__alpha': 200}\n"
     ]
    }
   ],
   "source": [
    "# the same as above but with standard scaler\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "ridge = Ridge()\n",
    "pipeline = Pipeline(steps=[('imputer', imputer),('scaler', StandardScaler()), ('ridge', ridge)])\n",
    "# Parameters of pipelines can be set using â€˜__â€™ separated parameter names:\n",
    "param_grid = {'ridge__alpha': [1e-15, 1e-4, 1, 20, 50, 100, 200]}\n",
    "param_grid = {'ridge__alpha': [1e-15, 1e-4, 1, 20]}\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid, n_jobs=-1, cv= 10)\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: ridge always want's the alpha to be as large as possible, i.e. it always chooses the largest possible alpha -> does this mean, the model is not fitting well? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) Ridge, cross validation, mean imputed, standard normalized, outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transform() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-e2a492dbd6f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best parameter (CV score=%0.3f):\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python_hs2020/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python_hs2020/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python_hs2020/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[1;32m    329\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    332\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/anaconda3/envs/python_hs2020/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    293\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python_hs2020/lib/python3.8/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python_hs2020/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python_hs2020/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: transform() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/52346725/can-i-add-outlier-detection-and-removal-to-scikit-learn-pipeline\n",
    "\n",
    "## Not working\n",
    "from sklearn.pipeline import Pipeline, TransformerMixin\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "class OutlierExtractor(TransformerMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Create a transformer to remove outliers. A threshold is set for selection\n",
    "        criteria, and further arguments are passed to the LocalOutlierFactor class\n",
    "\n",
    "        Keyword Args:\n",
    "            neg_conf_val (float): The threshold for excluding samples with a lower\n",
    "               negative outlier factor.\n",
    "\n",
    "        Returns:\n",
    "            object: to be used as a transformer method as part of Pipeline()\n",
    "        \"\"\"\n",
    "\n",
    "        self.threshold = kwargs.pop('neg_conf_val', -10.0)\n",
    "\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def transform(self, X, y):\n",
    "        \"\"\"\n",
    "        Uses LocalOutlierFactor class to subselect data based on some threshold\n",
    "\n",
    "        Returns:\n",
    "            ndarray: subsampled data\n",
    "\n",
    "        Notes:\n",
    "            X should be of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        lcf = LocalOutlierFactor(**self.kwargs)\n",
    "        lcf.fit(X)\n",
    "        return (X[lcf.negative_outlier_factor_ > self.threshold, :],\n",
    "                y[lcf.negative_outlier_factor_ > self.threshold])\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self\n",
    "    \n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "ridge = Ridge()\n",
    "pipeline = Pipeline(steps=[('imputer', imputer),('outliers', OutlierExtractor()),('scaler', StandardScaler()), ('ridge', ridge)])\n",
    "\n",
    "param_grid = {'ridge__alpha': [1e-15, 1e-4, 1, 20, 50, 100, 200]}\n",
    "param_grid = {'ridge__alpha': [1e-15, 1e-4, 1, 20]}\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid, n_jobs=-1, cv= 10)\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.233):\n",
      "{'ridge__alpha': 200}\n"
     ]
    }
   ],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "ridge = Ridge()\n",
    "pipeline = Pipeline(steps=[('imputer', imputer),('scaler', StandardScaler()), ('ridge', ridge)])\n",
    "\n",
    "param_grid = {'ridge__alpha': [1e-15, 1e-4, 1, 20, 50, 100, 200]}\n",
    "# param_grid = {'ridge__alpha': [1e-15, 1e-4, 1, 20]}\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid, n_jobs=-1, cv= 10)\n",
    "search.fit(X_train_outlierfree, y_train_outlierfree)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best found model\n",
    "\n",
    "For now the best found model is the last one: 3.) Ridge, cross validation, mean imputed, standard normalized, outlier detection.\n",
    "\n",
    "\n",
    "Fit model to whole training set and predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training set: 0.769847815885915\n"
     ]
    }
   ],
   "source": [
    "# 1.) impute missing values\n",
    "imputer = SimpleImputer(missing_values=np.nan , strategy='mean')\n",
    "X_train_val = X_train.values\n",
    "X_train_imputed = imputer.fit_transform(X_train_val)\n",
    "\n",
    "\n",
    "## Question: impute X_test with same immputer, or should we use other imputer fited to test data?\n",
    "## For now: use same\n",
    "X_test_imputed = imputer.transform(X_test.values)\n",
    "## Question: should X_test also be normalized? Or should normalization be done on Xtest and X_train together?\n",
    "## https://datascience.stackexchange.com/questions/53138/which-comes-first-multiple-imputation-splitting-into-train-test-or-standardiz\n",
    "## Performing pre-processing before splitting will mean that information from your \n",
    "## test set will be present during training, causing a data leak.\n",
    "##\n",
    "##The key here is that you are learning everything from the training set and then \"predicting\" on to the test set. \n",
    "\n",
    "# 2.) remove outliers\n",
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "outliers = lof.fit_predict(X_train_imputed)\n",
    "# select all rows that are not outliers\n",
    "mask = outliers != -1\n",
    "X, y = X_train_imputed[mask, :], y_train[mask]\n",
    "\n",
    "# Don't think I have to remove outliers of dataset, otherwise will have less rows in output...\n",
    "\n",
    "# 3.) normalize\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "X_test_norm = scaler.transform(X_test_imputed) # transforming with values learned on X_train\n",
    "\n",
    "ridge = Ridge(alpha=200)\n",
    "ridge.fit(X_norm, y)\n",
    "\n",
    "# prediction in train set\n",
    "y_train_pred = ridge.predict(X_norm)\n",
    "print(\"Score on training set: {}\".format(r2_score(y_true=y, y_pred= y_train_pred)))\n",
    "\n",
    "# predict on test set\n",
    "y_pred = ridge.predict(X_test_norm)\n",
    "\n",
    "\n",
    "# output\n",
    "output_csv = pd.concat([pd.Series(X_test.index.values), pd.Series(y_pred.flatten())], axis=1)\n",
    "output_csv.columns = [\"id\", \"y\"]\n",
    "\n",
    "pd.DataFrame.to_csv(output_csv, \"./data/submit.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_hs2020",
   "language": "python",
   "name": "python_hs2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
